%%%% ijcai09.tex

\typeout{Predicting NBA Draft Position for Basketball Prospects}

% These are the instructions for authors for IJCAI-09.
% They are the same as the ones for IJCAI-07 with superficical wording
%   changes only.

\documentclass{article}
% The file ijcai09.sty is the style file for IJCAI-09 (same as ijcai07.sty).
\usepackage{ijcai09}

% Use the postscript times font!
\usepackage{times}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

\usepackage{dblfloatfix} % To allow pushing table and figures to bottom of page
\usepackage{caption}
\usepackage{subcaption}

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Predicting the Draft Slot of NBA Prospects in the Modern Era}
\author{Kevin Hillyard \& Brent George \\
CS 472, Winter 2021 \\
Department of Computer Science\\
Brigham Young University}

\begin{document}

\maketitle

\begin{abstract}
  Every year experts attempt to determine which basketball players are the most
  valuable and when those players should be picked in the NBA draft. In this
  paper, we record our efforts to accurately predict where players should be
  drafted based on their stats using different machine learning models. We first
  describe by what means our data was gathered and what type of data we
  gathered. We then review the results we had in predicting a player’s draft
  position with our different models, namely KNN, decision tree, multi-layer
  perceptron, and linear regression. We then review our findings, especially
  from linear regression, in summarizing which statistics were important, and
  which were not as important. Lastly our ideas for future work and improvements
  to our research are made known.
\end{abstract}

\section{Introduction}

NBA scouts are always looking for NBA potential players. When determining how
good a player is they are almost always compared to past players. Most people
are able to predict the top 3 to 5 NBA picks from the available players but it
is nigh impossible to predict each draft pick accurately. 

In our research we attempted to use machine learning models to predict where a
player would be drafted. These models use the past 11 years of NBA draft
results, along with the players’ associated statistics, to determine where the
player should be drafted if they are to be drafted at all.

There was some difficulty in predicting a player’s draft position as it is a
human perception of that player’s capability and is often swayed by what each
NBA teams’ current needs are. We were also limited to a certain number of
players as the era of basketball has changed, encouraging more long range field
goals compared to years prior. This drastically affects the typical stats for
players making comparisons between newer and older players less viable.

Despite those difficulties we were able to predict a player’s draft position to
within 10 picks. In a future study if we were to account for the aforementioned
issues the accuracy of our results could be increased.

\section{Methods}

\subsection{Data Source}

There are various online providers of basketball statistics. Official sources
include NBA.com and the official site of NCAA basketball. However, official
sources often do not include more than the most basic statistics.

Several sites exist that provide extensive college statistics. Many of these
have a paywall (kenpom.com) or do not include statistics for international
prospects. Other sites request not to be scraped by bots.

After further exploration, we found realgm.com, a site that consented to limited
scraping that also included historical data for past statistics, data on
international prospects, and a wide array of advanced statistics. We wrote a web
scraper in Python to traverse the element tree of the web page and lift each of
the players selected in the past 11 NBA drafts and every available statistics
associated with those 660 players. This formed our initial dataset, which was
stored in a CSV file. We also pulled the records of nearly 736 prospects that
went undrafted during that same time period. 

\subsection{Featurization of Data}

Due to the nature of basketball statistics, the data scraped from the website
was already largely in the form we needed. We wrote a Python script to perform a
few sanitizing steps and convert the CSV file to an ARFF file.

The main sanitization we needed to perform was the consolidation of various
leagues that prospects played in. Prospects played in over 35 leagues and
countries. While there was consistency for prospects that played in the NCAA in
the United States, there was extreme fragmentation among the teams and leagues
that international prospects played for. In order to help the model, we grouped
all international teams under one label, leaving us with three total league
labels: NCAA, G-League, and International.

While drafted players were labeled according to their draft selection, the
undrafted players that we pulled were unlabeled. We didn’t want to use a
clustering algorithm to label these instances - a clustering algorithm would
attempt to label these players with a pick number from 1 to 60, whereas we
wanted to be able to predict if a player went undrafted. We decided to label any
undrafted player as being picked 61st, with the plan of interpreting our future
model’s output under this assumption. This led to over half of our dataset
consisting of players “selecting 61st in the draft”. This would prove to have
some positive effects, such as giving the model many examples of poor prospects
on the higher end of the draft. It would also have negative effects, such as
leading the models to be biased towards higher pick numbers when it was unsure. 

\subsection{Models}

We used a variety of models in this project, to be described in further detail
in results sections. All implementations came from scikit learn v1.0.1.

\section{Initial Results}

Initially, we trained Decision Tree, Multi-Layer Perceptron, KNN-Neighbor, and
Linear Regression models. Each of these had a grid of hyperparameters that was
exhaustively searched to find the best results, as recorded in Table
\ref{tab:test_table}.

\begin{table*}
	\begin{center}
		\begin{tabular}{||c c c c c c||} 
		\hline\hline
		Dataset & Algorithm & Centroid \# & SSE & Size & Silhouette Score \\ 
		\hline\hline
		Debug & Single Link HAC & Total & 54.4392 & 200 & 0.3453 \\ \hline
		Debug & Single Link HAC & 1 & 54.3917 & 195 & \\ \hline
		Debug & Single Link HAC & 2 & 0.0000 & 1 & \\ \hline
		Debug & Single Link HAC & 3 & 0.0475 & 2 & \\ \hline
		Debug & Single Link HAC & 4 & 0.0000 & 1 & \\ \hline
		Debug & Single Link HAC & 5 & 0.0000 & 1 & \\ \hline
		\hline\hline
		\end{tabular}
	\end{center}
	\caption{Test table}
	\label{tab:test_table}
\end{table*}

\subsection{Without Undrafted Players}
We trained our models on two separate versions of our dataset. One dataset did not include
the undrafted players. When training on this dataset the Mean Absolute Error (MAE) for each model was higher 
when compared to the models trained on the entire dataset. An issue we found with this subset was that there
were not enough "bad" players for the models to learn with.
\subsection{Decision Tree}
After training the decision tree model on this subset it achieved a MAE of 13.689. 
We trained the model with max depths of either 8, 16, 24, 32, or 48 layers. In addition we also used a minimum sample size of either 2, 4, or 6 samples 
for splits and for leaves. Lastly we set the minimum weight fraction leaf and the minimum impurity decrease to values of either 0, 0.01, or 0.001.
The hyperparameters with which we achieved the best score had max depth set to 8 layers, minimum samples set to 6 and 4 for split and leaf respectively, and both the minimum weight fraction leaf and minimum impurity decrease set to 0.01.
\subsection{KNN-Regressor}
When training our KNN-Regressor on the subset excluding undrafted players it achieved a MAE of 13.129.
During the training of this model we used odd values from 1 tp 11 for k and a distance metric of either manhattan, chebyshev, euclidean, or minkowski.
We achieved the best MAE when setting k to 9 and using a manhattan distance metric.
\subsection{Linear Regression}
The MAE our Linear Regression model achieved was a value of 11.265.
No special hyperparameters were used for this model.
\subscetion{MLP}
The last model which we trained on this subset of data was our MLP. It achieved a MAE of 11.439.
When training this model we used the following values for hidden nodes: 32, 64, 128, (16,16), (16,32,16), (32,32), (8,16,8), (8,8), and (8,8,8).
We also set the learning rate to 0.1 or 0.2. Momentum was set to a value of either 0.3, 0.5, or 0.8. 
Lastly we used Regularization values of 0.0001, 0.0005, or 0.001.
The combination of hyperparameters that achieved the best MAE had hidden nodes set to (8,8), a learning rate of 0.1,
a momentum of 0.5, and regularization of 0.0001.

\subsection{With Drafted and Undrafted Players}
As already mentioned the following models achieved a better MAE when using both drafted and undafted players. 
The variety of talent allowed the models to learn to recognize both good and bad players and was more accurate as a result.
\subsection{Decision Tree}
When training the decision tree on both drafted and undrafted players it achieved a MAE of 9.560. 
This is an improvement from 13.689 MAE that was achieved using the subset without undrafted players.
This score would prove to be our best initial score. We used the same hyperparameters mentioned before and the
best score was achieved when using the following settings: max depth was set to 8 layers, 
the min samples for splits was set to 4 while the min samples for leaves was set to 1, the minimum weight fraction leaf 
was set to 0.01, and the minimum impurity decrease was set to 0.001.
\subsection{KNN-Regressor}
The KNN-Regressor when trained on both drafted and undrafted players achieved a MAE of 11.699. This is 1.43 better than 
the previous model. This score would be the worst initial score for all models trained on both drafted and undrafted players.
The score was achieved using the same hyperparameters as before but with a k value of 3 using a euclidean distance metric.
\subsection{Linear Regression}
Our Linear Regression model had the third best initial score with a MAE of 10.091. This is an improvement from the previous 
Linear Regression model that scored 11.265, but is in the bottom half of models trained on both drafted and undrafted players.
\subsection{MLP}
Lastly our MLP had the second best initial score with a MAE of 9.851. It used the same hyperparameters as the previous MLP model, 
but had the following settings: hidden nodes was set to 128, learning rate was set to 0.1, momentum was set to 0.8, and we used a 
regularization value of 0.0001. Compared to the first MLP model not trained on undrafted players, this model improved that score by 1.588.

\section{Feature and Model Improvements}

\subsection{Improvements}

To improve our results we reviewed the most important features and least
important features as determined by the weights from our linear regression
model. The features that had the lowest weights included the following features:
Usage \%, Assist \%, Field Goals Made, Field Goals Attempted, Free Throws Made,
Free Throws Attempted, 3-pointers Made, and 3-pointers Attempted. We deemed
these features safe to remove as they had low weights and the essence of their
statistic was captured in a different feature. For example, Field Goals Made and
Field Goals Attempted is captured in the feature Field Goal \% which has a high
weight. In removing these somewhat redundant features we were able to slightly
improve our predictions. 


\section{Final Results}

\section{Discussion and Conclusion}

\section{Future Work}

There are some minor changes we would make to our research to improve the
results of our predictions. These changes include further research about the
data, how the error of our models was determined, and some additional features.

\subsection{Further Draft Research}

As previously mentioned in this report there were some contributing factors to a
player’s draft stock that we did not take into account when gathering our data.
When gathering our data and when a player was drafted it would be worthwhile to
research the team’s needs that drafted that player. Knowing this would allow us
to determine if the player was drafted at that position for their skills as a
player or more for their fit with that team.

\subsection{Error Scoring}

There was an issue with the error scoring with our models as we were using the
scikit models with their own error scoring function. Because undrafted players
don’t have a draft position associated with them we edited our data so that all
undrafted players were drafted 61st. The issue appears when our models predict a
draft position greater than 61. Our data has a max of 61 while our models are
able to predict draft positions higher than this. The error scoring considers
this difference as an error when in reality if the player has a draft position
of 61 any placement greater than or equal to 61 should be considered correct. 

\subsection{Additional Features/Sources}

As a final change to our research we would gather data from one or more
additional sites in order to gather all advanced statistics possible. The site
we pulled our data from had a large number of advanced statistics but not all of
them.


\end{document}

